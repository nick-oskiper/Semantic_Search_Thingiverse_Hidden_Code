{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Only the main process_and_describe function is shown.\n",
    "The code below (generate_description_main.py) takes 3D models' downloaded files from the Thingiverse scraper and uses GenAI to generate:\n",
    "1. a description using either provided image files or blender snapshots\n",
    "2. A category for the model\n",
    "3. tags for the model\n",
    "\n",
    "This outputs a csv with the generated information for each model and can be paused at any time (since it costs money).\n",
    "This can be skipped as there are around 40 models in the csv, but can be run if more want to be loaded."
   ],
   "id": "33d64d6d72d07114"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-07T17:55:56.362128Z",
     "start_time": "2025-01-07T17:55:56.348796Z"
    }
   },
   "source": [
    "# def process_and_describe(client, repo_owner, repo_name, local_extract_path, subdir, csv_writer, existing_ids, save_images=True):\n",
    "#     \"\"\"\n",
    "#     Processes and describes each model in the repository.\n",
    "#\n",
    "#     Parameters:\n",
    "#     - client (OpenAIClient): Instance of OpenAIClient.\n",
    "#     - repo_owner (str): Owner of the repository.\n",
    "#     - repo_name (str): Name of the repository.\n",
    "#     - local_extract_path (str): Path where files are extracted.\n",
    "#     - subdir (dict): Information about the subdirectory.\n",
    "#     - csv_writer (csv.writer): CSV writer object.\n",
    "#     - existing_ids (set): Set of already processed object IDs.\n",
    "#     - save_images (bool): Whether to save processed images.\n",
    "#     \"\"\"\n",
    "#     subdir_path = os.path.join(local_extract_path, subdir['path'])\n",
    "#     os.makedirs(subdir_path, exist_ok=True)\n",
    "#\n",
    "#     try:\n",
    "#         files = list_files_in_repo(repo_owner, repo_name, subdir['path'])\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error listing files in subdir {subdir['path']}: {e}\")\n",
    "#         return\n",
    "#\n",
    "#     if not files:\n",
    "#         print(\"No files found in the repository.\")\n",
    "#         return\n",
    "#\n",
    "#     print(f\"Found {len(files)} items in {subdir['path']} directory.\")\n",
    "#     for file_info in files:\n",
    "#         if file_info['type'] == 'file' and file_info['name'].endswith('.zip'):\n",
    "#             zip_path = os.path.join(subdir_path, file_info['name'])  # Path of zip file\n",
    "#             try:\n",
    "#                 download_file(file_info['download_url'], zip_path)  # Downloads file at path\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Skipping file due to download error: {zip_path} - {e}\")\n",
    "#                 continue\n",
    "#\n",
    "#             extract_zip(zip_path, subdir_path)  # Extracts zip to subdir\n",
    "#             images_dir = os.path.join(subdir_path, 'images')\n",
    "#             image_files = find_image_files(images_dir)  # Finds all images for each item\n",
    "#\n",
    "#             # Define the exclusion list\n",
    "#             BROAD_TAGS_EXCLUSION = [\n",
    "#                 \"3d model\", \"fully assembled\", \"complete\", \"model\", \"assembly\",\n",
    "#                 \"thing\", \"object\", \"product\", \"item\", \"prototype\"\n",
    "#             ]\n",
    "#\n",
    "#             if not image_files:\n",
    "#                 print(f\"No image files found in {zip_path}. Attempting to render using Blender.\")\n",
    "#                 # Locate the .stl or .obj file under 'files/' subdirectory\n",
    "#                 files_subdir = os.path.join(subdir_path, 'files')\n",
    "#                 if not os.path.isdir(files_subdir):\n",
    "#                     print(f\"No 'files/' directory found in {zip_path}. Skipping.\")\n",
    "#                     continue\n",
    "#\n",
    "#                 # Find all .stl and .obj files in 'files/' directory\n",
    "#                 model_files = [f for f in os.listdir(files_subdir) if f.lower().endswith(('.stl', '.obj'))]\n",
    "#                 if not model_files:\n",
    "#                     print(f\"No .stl or .obj files found in 'files/' directory of {zip_path}. Skipping.\")\n",
    "#                     continue\n",
    "#\n",
    "#                 # For simplicity, process the first .stl/.obj file found\n",
    "#                 model_file = os.path.join(files_subdir, model_files[0])\n",
    "#                 print(f\"Found model file: {model_file}\")\n",
    "#\n",
    "#                 # Define paths for Blender executable and script\n",
    "#                 blender_executable = r\"C:\\Program Files\\Blender Foundation\\Blender 4.2\\blender.exe\"  # Update if different\n",
    "#                 blender_script = \"\"\n",
    "#                 rendered_image_path = os.path.join(subdir_path, 'rendered_image.png')\n",
    "#\n",
    "#                 # Call Blender to render the image\n",
    "#                 rendered_image = render_model_with_blender(\n",
    "#                     blender_executable,\n",
    "#                     blender_script,\n",
    "#                     model_file,\n",
    "#                     rendered_image_path\n",
    "#                 )\n",
    "#\n",
    "#                 if rendered_image and os.path.exists(rendered_image):\n",
    "#                     image_to_use = rendered_image\n",
    "#                 else:\n",
    "#                     print(f\"Failed to render image for {zip_path}. Skipping.\")\n",
    "#                     continue\n",
    "#             else:\n",
    "#                 # Use the largest image file if no combined image is found\n",
    "#                 image_to_use = find_largest_image_file(image_files)\n",
    "#                 if not image_to_use:\n",
    "#                     print(f\"No suitable image found in {zip_path}\")\n",
    "#                     continue\n",
    "#\n",
    "#             # Log the image being processed\n",
    "#             print(f\"Processing image: {image_to_use}\")\n",
    "#\n",
    "#             # Extract keywords from the zip file name and add new keywords\n",
    "#             zip_file_name = os.path.splitext(file_info['name'])[0]\n",
    "#             keywords = zip_file_name.replace('-', ' ').replace('_', ' ').split()\n",
    "#             keywords.extend([\"complete\", \"assembled\", \"whole\", \"IMG_\"])  # Keywords include name and these\n",
    "#\n",
    "#             # Generate initial guess based on the directory name\n",
    "#             initial_guess = \" \".join(keywords).capitalize()\n",
    "#\n",
    "#             # Extract object_id from subdir_path\n",
    "#             id = os.path.basename(subdir_path)\n",
    "#\n",
    "#             # Check if object_id already exists\n",
    "#             if id in existing_ids:\n",
    "#                 print(f\"Object ID {id} already exists in CSV. Skipping.\")\n",
    "#                 continue\n",
    "#\n",
    "#             # Generate description, category, and tags using OpenAI\n",
    "#             ai_data = client.describe_image_model(image_to_use, initial_guess)\n",
    "#             if ai_data:\n",
    "#                 description = ai_data.get('description', '').strip()\n",
    "#                 category = ai_data.get('category', '').strip()\n",
    "#                 tags = ai_data.get('tags', [])\n",
    "#\n",
    "#                 if not description or not category or not tags:\n",
    "#                     print(f\"Incomplete AI data for {image_to_use}. Skipping.\")\n",
    "#                     continue\n",
    "#\n",
    "#                 # Filter out broad/generic tags\n",
    "#                 filtered_tags = filter_tags(tags, BROAD_TAGS_EXCLUSION)\n",
    "#\n",
    "#                 # If no tags remain after filtering, assign a default tag\n",
    "#                 if not filtered_tags:\n",
    "#                     print(f\"All tags for {image_to_use} were broad. Assigning default tag 'unknown'.\")\n",
    "#                     filtered_tags = [\"unknown\"]\n",
    "#\n",
    "#                 # Format tags as comma-separated string\n",
    "#                 tags_str = \", \".join(filtered_tags)\n",
    "#\n",
    "#                 print(f\"Description for {image_to_use}:\\n{description}\\n\")\n",
    "#                 print(f\"Category: {category}\")\n",
    "#                 print(f\"Tags: {tags_str}\\n\")\n",
    "#\n",
    "#                 # Derive name from the zip file name or any other logic\n",
    "#                 name = \" \".join(keywords).capitalize()\n",
    "#\n",
    "#                 # Write object_id, description, name, category, tags to CSV\n",
    "#                 csv_writer.writerow([id, description, name, category, tags_str])\n",
    "#                 print(f\"Wrote data for Object ID {id} to CSV.\\n\")\n",
    "#\n",
    "#                 # Add to existing_ids to prevent reprocessing within the same run\n",
    "#                 existing_ids.add(id)\n",
    "#\n",
    "#             # Save a copy of the image being processed for visual inspection if save_images is True\n",
    "#             if save_images:\n",
    "#                 try:\n",
    "#                     output_dir = os.path.join(local_extract_path, 'processed_images')  # Puts them to processed_images\n",
    "#                     os.makedirs(output_dir, exist_ok=True)\n",
    "#                     img_output_path = os.path.join(output_dir, os.path.basename(image_to_use))  # Good for verifying correctness of GPT\n",
    "#\n",
    "#                     # Ensure path length is within acceptable limits\n",
    "#                     if len(img_output_path) > 255:\n",
    "#                         print(f\"Path too long: {img_output_path}\")\n",
    "#                         continue\n",
    "#\n",
    "#                     # Save the image file\n",
    "#                     with open(image_to_use, 'rb') as img_file:\n",
    "#                         with open(img_output_path, 'wb') as output_file:\n",
    "#                             output_file.write(img_file.read())\n",
    "#                     print(f\"Saved processed image to: {img_output_path}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error saving image {image_to_use}: {e}\")\n",
    "#\n",
    "#             # Pause to manage load\n",
    "#             time.sleep(1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, this code uses the AI generated summaries above, as well as the nonAI summaries from the thingiverse Scraper. It constructs a knowledge graph with the following edges between each node: \n",
    "1. AI and nonAI summaries similarity using tf-idf vectors and cosine similarity calculation (labeled SIMILAR_SUMMARY_AI and SIMILAR_SUMMARY_NONAI)\n",
    "2. AI and nonAI summaries similarity using BERT Embedding (labeled SIMILAR_EMBEDDING_AI and SIMILAR_EMBEDDING_NONAI)\n",
    "3. Name similarity using tf-idf vectors and cosine similarity calculation (labeled SIMILAR_NAMES_ALL)\n",
    "4. AI and nonAI tag similarity using Jaccard similarity (labeled SIMILAR_TAGS_AI and SIMILAR_TAGS_NONAI)\n",
    "\n",
    "### Only the main function and output are shown.\n",
    "\n",
    "Because there are much fewer AI description models than nonAI description models, right now only about 540 models have similarity edges based on AI calculations. However all models have nonAI elements and name edges between nodes. This creates a knoweldge graph with 7 edges, 4 of which are present in every single models and 3 of which are only present in about 40 models. However, with more descriptions generated, more AI based similarity edges can be created."
   ],
   "id": "ce98aa67163701d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T19:14:26.339074Z",
     "start_time": "2024-11-26T18:16:54.608716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def main(csv_ai_path, csv_nonai_path, k_threshold=True, k=1):\n",
    "#     logging.info(\"Starting semantic network construction process.\")\n",
    "#\n",
    "#     # Step 1: Connect to Neo4j\n",
    "#     graph_db = connect_to_neo4j(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "#\n",
    "#     # Step 2: Load Data\n",
    "#     df_nonai, df_ai = load_data(csv_ai_path, csv_nonai_path)\n",
    "#\n",
    "#     # Step 3: Preprocess Data\n",
    "#     df_nonai = preprocess_dataframe(df_nonai, is_ai=False)\n",
    "#     df_ai = preprocess_dataframe(df_ai, is_ai=True)\n",
    "#\n",
    "#     # Step 3.1: Verification - Ensure all models have 'name'\n",
    "#     num_nonai_with_name = df_nonai['name'].notna().sum()\n",
    "#     num_ai_with_name = df_ai['name'].notna().sum()\n",
    "#     total_models = df_nonai.shape[0] + df_ai.shape[0]\n",
    "#\n",
    "#     logging.info(f\"Non-AI models with extracted names: {num_nonai_with_name}/{df_nonai.shape[0]}\")\n",
    "#     logging.info(f\"AI models with names: {num_ai_with_name}/{df_ai.shape[0]}\")\n",
    "#     logging.info(f\"Total models with names: {num_nonai_with_name + num_ai_with_name}/{total_models}\")\n",
    "#\n",
    "#     if (num_nonai_with_name + num_ai_with_name) < total_models:\n",
    "#         logging.warning(\"Some models do not have a name extracted. They will not be included in 'SIMILAR_NAMES' relationships.\")\n",
    "#\n",
    "#     # Step 4: Extract Key Components\n",
    "#     df_nonai = extract_key_nouns_adjectives(df_nonai, is_ai=False)\n",
    "#     df_ai = extract_key_nouns_adjectives(df_ai, is_ai=True)\n",
    "#\n",
    "#     # Step 5: Expand Synonyms\n",
    "#     df_nonai, synonym_dict_nonai = expand_synonyms(df_nonai, is_ai=False)\n",
    "#     df_ai, synonym_dict_ai = expand_synonyms(df_ai, is_ai=True)\n",
    "#\n",
    "#     # Step 6: Perform Named Entity Recognition\n",
    "#     df_nonai = perform_ner(df_nonai, is_ai=False)\n",
    "#     df_ai = perform_ner(df_ai, is_ai=True)\n",
    "#\n",
    "#     # Step 7: Weight primary nouns and named entities, incorporating synonyms in descriptions\n",
    "#     df_nonai = duplicate_primary_nouns(df_nonai, is_ai=False, weight=2.0)\n",
    "#     df_ai = duplicate_primary_nouns(df_ai, is_ai=True, weight=2.0)\n",
    "#\n",
    "#     # Step 8: Text Similarity Calculation\n",
    "#     logging.info(\"Calculating similarity metrics...\")\n",
    "#\n",
    "#     # Non-AI Similarities\n",
    "#     tfidf_nonai, vectorizer_nonai = compute_tfidf_vectors(df_nonai, description_type='nonai')\n",
    "#     cosine_sim_nonai = compute_cosine_similarity_matrix(tfidf_nonai)\n",
    "#     embedding_sim_nonai = compute_sentence_transformer_similarity(df_nonai, description_type='nonai')\n",
    "#\n",
    "#     # AI Similarities (only among AI models)\n",
    "#     if not df_ai.empty and df_ai['clean_description_ai'].str.strip().any():\n",
    "#         tfidf_ai, vectorizer_ai = compute_tfidf_vectors(df_ai, description_type='ai')\n",
    "#         cosine_sim_ai = compute_cosine_similarity_matrix(tfidf_ai)\n",
    "#         embedding_sim_ai = compute_sentence_transformer_similarity(df_ai, description_type='ai')\n",
    "#     else:\n",
    "#         logging.warning(\"No AI descriptions available. Skipping AI similarity computations.\")\n",
    "#         cosine_sim_ai = np.array([])\n",
    "#         embedding_sim_ai = np.array([])\n",
    "#\n",
    "#     # Compute tag similarity for Non-AI models\n",
    "#     tag_sim_nonai = compute_tag_similarity(df_nonai, is_ai=False)\n",
    "#\n",
    "#     # Compute tag similarity for AI models\n",
    "#     if not df_ai.empty:\n",
    "#         tag_sim_ai = compute_tag_similarity(df_ai, is_ai=True)\n",
    "#     else:\n",
    "#         tag_sim_ai = np.array([])\n",
    "#\n",
    "#     # Compute name similarity for all models (both AI and Non-AI)\n",
    "#     logging.info(\"Computing name similarity for all models...\")\n",
    "#     combined_df_names = pd.concat([\n",
    "#         df_nonai[['id', 'name']],\n",
    "#         df_ai[['id', 'name']]\n",
    "#     ], ignore_index=True)\n",
    "#\n",
    "#     # Remove models without a name\n",
    "#     combined_df_names = combined_df_names[combined_df_names['name'].str.strip() != ''].reset_index(drop=True)\n",
    "#\n",
    "#     if not combined_df_names.empty:\n",
    "#         cosine_sim_name = compute_names_similarity_matrix(combined_df_names['name'].tolist())\n",
    "#     else:\n",
    "#         cosine_sim_name = np.array([])\n",
    "#         logging.warning(\"No names available to compute name similarity.\")\n",
    "#\n",
    "#     # Similarity matrices\n",
    "#     similarity_matrices_nonai = {\n",
    "#         \"SIMILAR_SUMMARY_NONAI\": cosine_sim_nonai,\n",
    "#         \"SIMILAR_TAGS_NONAI\": tag_sim_nonai,\n",
    "#         \"SIMILAR_EMBEDDING_NONAI\": embedding_sim_nonai\n",
    "#     }\n",
    "#\n",
    "#     similarity_matrices_ai = {}\n",
    "#     if not df_ai.empty and df_ai['clean_description_ai'].str.strip().any():\n",
    "#         similarity_matrices_ai = {\n",
    "#             \"SIMILAR_SUMMARY_AI\": cosine_sim_ai,\n",
    "#             \"SIMILAR_TAGS_AI\": tag_sim_ai,\n",
    "#             \"SIMILAR_EMBEDDING_AI\": embedding_sim_ai\n",
    "#         }\n",
    "#     else:\n",
    "#         logging.warning(\"AI similarity matrices are empty.\")\n",
    "#\n",
    "#     # Add name similarity for all models if available\n",
    "#     if cosine_sim_name.size > 0:\n",
    "#         similarity_matrices_nonai[\"SIMILAR_NAMES_ALL\"] = cosine_sim_name\n",
    "#\n",
    "#     # Determine dynamic thresholds if enabled\n",
    "#     thresholds_nonai = {}\n",
    "#     thresholds_ai = {}\n",
    "#     thresholds_all_names = {}\n",
    "#     if k_threshold:\n",
    "#         for rel_type, sim_matrix in similarity_matrices_nonai.items():\n",
    "#             if sim_matrix.size > 0:\n",
    "#                 threshold = determine_threshold_mean_std(sim_matrix, k=k)\n",
    "#                 thresholds_nonai[rel_type] = threshold\n",
    "#\n",
    "#         for rel_type, sim_matrix in similarity_matrices_ai.items():\n",
    "#             if sim_matrix.size > 0:\n",
    "#                 threshold = determine_threshold_mean_std(sim_matrix, k=k)\n",
    "#                 thresholds_ai[rel_type] = threshold\n",
    "#\n",
    "#         if \"SIMILAR_NAMES_ALL\" in similarity_matrices_nonai:\n",
    "#             threshold = determine_threshold_mean_std(similarity_matrices_nonai[\"SIMILAR_NAMES_ALL\"], k=k)\n",
    "#             thresholds_nonai[\"SIMILAR_NAMES_ALL\"] = threshold\n",
    "#     else:\n",
    "#         thresholds_nonai = {rel_type: 0.0 for rel_type in similarity_matrices_nonai.keys()}\n",
    "#         thresholds_ai = {rel_type: 0.0 for rel_type in similarity_matrices_ai.keys()}\n",
    "#         if \"SIMILAR_NAMES_ALL\" in similarity_matrices_nonai:\n",
    "#             thresholds_nonai[\"SIMILAR_NAMES_ALL\"] = 0.0\n",
    "#\n",
    "#     # Visualize similarity distribution with threshold\n",
    "#     # for rel_type, sim_matrix in similarity_matrices_nonai.items():\n",
    "#     #     threshold = thresholds_nonai.get(rel_type, None) if k_threshold else None\n",
    "#     #     visualize_similarity_distribution(\n",
    "#     #         sim_matrix,\n",
    "#     #         relationship_type=rel_type,\n",
    "#     #         threshold=threshold\n",
    "#     #     )\n",
    "#     #\n",
    "#     # for rel_type, sim_matrix in similarity_matrices_ai.items():\n",
    "#     #     threshold = thresholds_ai.get(rel_type, None) if k_threshold else None\n",
    "#     #     visualize_similarity_distribution(\n",
    "#     #         sim_matrix,\n",
    "#     #         relationship_type=rel_type,\n",
    "#     #         threshold=threshold\n",
    "#     #     )\n",
    "#\n",
    "#     # Step 9: Knowledge Graph Construction with Embeddings\n",
    "#     logging.info(\"Constructing knowledge graph with embeddings...\")\n",
    "#     create_nodes_with_embeddings(graph_db, df_nonai, is_ai=False)\n",
    "#     create_nodes_with_embeddings(graph_db, df_ai, is_ai=True)\n",
    "#\n",
    "#     # Step 10: Create Edges Based on Similarity\n",
    "#     create_edges_batch(\n",
    "#         graph_db,\n",
    "#         df_nonai,\n",
    "#         similarity_matrices_nonai,\n",
    "#         thresholds_nonai,\n",
    "#         is_ai=False,\n",
    "#         batch_size=1000\n",
    "#     )\n",
    "#\n",
    "#     create_edges_batch(\n",
    "#         graph_db,\n",
    "#         df_ai,\n",
    "#         similarity_matrices_ai,\n",
    "#         thresholds_ai,\n",
    "#         is_ai=True,\n",
    "#         batch_size=1000\n",
    "#     )\n",
    "#\n",
    "#     # Step 11: Additional Optimizations\n",
    "#     # Define all relationship types for pruning\n",
    "#     relationship_types_nonai = list(similarity_matrices_nonai.keys())\n",
    "#     relationship_types_ai = list(similarity_matrices_ai.keys())\n",
    "#\n",
    "#     # Prune edges below the dynamic threshold for each relationship type\n",
    "#     graph_pruning(\n",
    "#         graph_db,\n",
    "#         relationship_types_nonai + relationship_types_ai,\n",
    "#         thresholds={**thresholds_nonai, **thresholds_ai}\n",
    "#     )\n",
    "#\n",
    "#     # Perform clustering for each similarity type\n",
    "#     for rel_type in similarity_matrices_nonai.keys():\n",
    "#         description_type = rel_type.lower().replace(\"similar_\", \"\")\n",
    "#         similarity_matrix = similarity_matrices_nonai[rel_type]\n",
    "#         threshold = thresholds_nonai.get(rel_type, 0.0)\n",
    "#         node_clustering(\n",
    "#             graph_db,\n",
    "#             df_nonai,\n",
    "#             similarity_matrix,\n",
    "#             threshold=threshold,\n",
    "#             description_type=description_type\n",
    "#         )\n",
    "#\n",
    "#     for rel_type in similarity_matrices_ai.keys():\n",
    "#         description_type = rel_type.lower().replace(\"similar_\", \"\")\n",
    "#         similarity_matrix = similarity_matrices_ai[rel_type]\n",
    "#         threshold = thresholds_ai.get(rel_type, 0.0)\n",
    "#         node_clustering(\n",
    "#             graph_db,\n",
    "#             df_ai,\n",
    "#             similarity_matrix,\n",
    "#             threshold=threshold,\n",
    "#             description_type=description_type\n",
    "#         )\n",
    "#\n",
    "#     # Step 12: Create Similar Names Relationships for All Models\n",
    "#     logging.info(\"Creating 'SIMILAR_NAMES' relationships based on extracted names...\")\n",
    "#\n",
    "#     # Combine AI and Non-AI data for names\n",
    "#     combined_df = pd.concat([\n",
    "#         df_nonai[['id', 'name']],\n",
    "#         df_ai[['id', 'name']]\n",
    "#     ], ignore_index=True)\n",
    "#\n",
    "#     # Remove duplicates based on name\n",
    "#     name_groups = combined_df.groupby('name')['id'].apply(list).reset_index()\n",
    "#\n",
    "#     # Create 'similar_names' relationships where models share the same name\n",
    "#     similar_names_relationships = []\n",
    "#     for _, row in tqdm(name_groups.iterrows(), total=name_groups.shape[0], desc=\"Processing Similar Names\"):\n",
    "#         model_ids = row['id']\n",
    "#         if len(model_ids) > 1 and row['name']:\n",
    "#             for combo in combinations(model_ids, 2):\n",
    "#                 similar_names_relationships.append({\n",
    "#                     \"source_id\": combo[0],\n",
    "#                     \"target_id\": combo[1],\n",
    "#                     \"score\": 1.0  # Assuming exact match\n",
    "#                 })\n",
    "#\n",
    "#     # Insert 'SIMILAR_NAMES' relationships in batches\n",
    "#     batch_size = 1000\n",
    "#     for i in tqdm(range(0, len(similar_names_relationships), batch_size), desc=\"Inserting 'SIMILAR_NAMES' Relationships\"):\n",
    "#         batch = similar_names_relationships[i:i + batch_size]\n",
    "#         merge_relationships_batch(graph_db, batch, \"SIMILAR_NAMES\")\n",
    "#\n",
    "#     logging.info(f\"Inserted total 'SIMILAR_NAMES' relationships: {len(similar_names_relationships)}\")\n",
    "#\n",
    "#     logging.info(\"Knowledge graph construction complete.\")\n",
    "#\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set k_threshold=True to enable dynamic thresholding\n",
    "#     # Set k_threshold=False to disable dynamic thresholding\n",
    "#     # Adjust k as needed (e.g., k=1 for mean + 1*std)\n",
    "#     main(\n",
    "#         csv_ai_path=CSV_AI_PATH,\n",
    "#         csv_nonai_path=CSV_NONAI_PATH,\n",
    "#         k_threshold=True,\n",
    "#         k=2,\n",
    "#     )"
   ],
   "id": "e02ab9fd1038ba25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noski\\PycharmProjects\\Semantic_Search_Thingiverse\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\noski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\noski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\noski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-11-26 13:17:01,192 [INFO] SpaCy Pipeline Components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "2024-11-26 13:17:01,200 [INFO] Starting semantic network construction process.\n",
      "2024-11-26 13:17:01,734 [INFO] Successfully connected to Neo4j.\n",
      "2024-11-26 13:17:01,760 [INFO] Non-AI CSV Columns: ['id', 'summary']\n",
      "2024-11-26 13:17:01,769 [INFO] AI CSV Columns: ['id', 'description', 'name', 'category', 'tags']\n",
      "2024-11-26 13:17:01,773 [INFO] Number of AI models: 539\n",
      "2024-11-26 13:17:01,774 [INFO] Number of Non-AI models: 978\n",
      "2024-11-26 13:17:01,775 [INFO] Starting text preprocessing for Non-AI models...\n",
      "2024-11-26 13:17:01,776 [INFO] Extracting tags from Non-AI summaries...\n",
      "2024-11-26 13:17:01,803 [INFO] Extracting names from Non-AI summaries...\n",
      "2024-11-26 13:17:01,843 [INFO] Preprocessing Non-AI summaries...\n",
      "Preprocessing Non-AI Text: 100%|██████████| 978/978 [00:12<00:00, 75.85it/s] \n",
      "2024-11-26 13:17:14,742 [INFO] Completed text preprocessing for Non-AI models.\n",
      "2024-11-26 13:17:14,742 [INFO] Starting text preprocessing for AI models...\n",
      "Preprocessing AI Text: 100%|██████████| 539/539 [00:03<00:00, 146.20it/s]\n",
      "2024-11-26 13:17:18,435 [INFO] Completed text preprocessing for AI models.\n",
      "2024-11-26 13:17:18,436 [INFO] Non-AI models with extracted names: 978/978\n",
      "2024-11-26 13:17:18,437 [INFO] AI models with names: 539/539\n",
      "2024-11-26 13:17:18,437 [INFO] Total models with names: 1517/1517\n",
      "2024-11-26 13:17:18,438 [INFO] Extracting key nouns and adjectives for Non-AI models...\n",
      "Processing Rows: 100%|██████████| 978/978 [00:10<00:00, 89.67it/s] \n",
      "2024-11-26 13:17:29,347 [INFO] Completed extraction of key nouns and adjectives for Non-AI models.\n",
      "2024-11-26 13:17:29,348 [INFO] Extracting key nouns and adjectives for AI models...\n",
      "Processing Rows: 100%|██████████| 539/539 [00:03<00:00, 159.21it/s]\n",
      "2024-11-26 13:17:32,736 [INFO] Completed extraction of key nouns and adjectives for AI models.\n",
      "2024-11-26 13:17:32,737 [INFO] Expanding nouns and adjectives with filtered synonyms for Non-AI models...\n",
      "Generating Synonyms: 100%|██████████| 6358/6358 [00:02<00:00, 2378.59it/s]\n",
      "2024-11-26 13:17:35,471 [INFO] Completed expanding synonyms for Non-AI models.\n",
      "2024-11-26 13:17:35,472 [INFO] Expanding nouns and adjectives with filtered synonyms for AI models...\n",
      "Generating Synonyms: 100%|██████████| 2346/2346 [00:00<00:00, 51452.28it/s]\n",
      "2024-11-26 13:17:35,543 [INFO] Completed expanding synonyms for AI models.\n",
      "2024-11-26 13:17:35,544 [INFO] Performing Named Entity Recognition (NER) for Non-AI models...\n",
      "Processing NER: 100%|██████████| 978/978 [00:11<00:00, 88.85it/s] \n",
      "2024-11-26 13:17:46,556 [INFO] Completed NER for Non-AI models.\n",
      "2024-11-26 13:17:46,557 [INFO] Performing Named Entity Recognition (NER) for AI models...\n",
      "Processing NER: 100%|██████████| 539/539 [00:03<00:00, 163.82it/s]\n",
      "2024-11-26 13:17:49,850 [INFO] Completed NER for AI models.\n",
      "2024-11-26 13:17:49,851 [INFO] Duplicating primary nouns and named entities with weight=2.0 for Non-AI models...\n",
      "2024-11-26 13:17:49,881 [INFO] Completed duplicating primary nouns and named entities for Non-AI models.\n",
      "2024-11-26 13:17:49,882 [INFO] Duplicating primary nouns and named entities with weight=2.0 for AI models...\n",
      "2024-11-26 13:17:49,893 [INFO] Completed duplicating primary nouns and named entities for AI models.\n",
      "2024-11-26 13:17:49,894 [INFO] Calculating similarity metrics...\n",
      "2024-11-26 13:17:49,895 [INFO] Computing TF-IDF vectors on weighted descriptions (nonai)...\n",
      "2024-11-26 13:17:50,114 [INFO] Completed TF-IDF computation.\n",
      "2024-11-26 13:17:50,114 [INFO] Computing cosine similarity matrix...\n",
      "2024-11-26 13:17:50,166 [INFO] Completed cosine similarity computation.\n",
      "2024-11-26 13:17:50,167 [INFO] Computing Sentence-BERT similarity (nonai)...\n",
      "2024-11-26 13:17:50,172 [INFO] Use pytorch device_name: cpu\n",
      "2024-11-26 13:17:50,173 [INFO] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "C:\\Users\\noski\\PycharmProjects\\Semantic_Search_Thingiverse\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b33d8471f69436fb910bcb7cba4d08a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 13:18:08,146 [INFO] Completed Sentence-BERT similarity computation.\n",
      "2024-11-26 13:18:08,147 [INFO] Computing TF-IDF vectors on weighted descriptions (ai)...\n",
      "2024-11-26 13:18:08,218 [INFO] Completed TF-IDF computation.\n",
      "2024-11-26 13:18:08,219 [INFO] Computing cosine similarity matrix...\n",
      "2024-11-26 13:18:08,235 [INFO] Completed cosine similarity computation.\n",
      "2024-11-26 13:18:08,237 [INFO] Computing Sentence-BERT similarity (ai)...\n",
      "2024-11-26 13:18:08,242 [INFO] Use pytorch device_name: cpu\n",
      "2024-11-26 13:18:08,243 [INFO] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2b7e2ba00d549119def62e9e498067a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 13:18:19,051 [INFO] Completed Sentence-BERT similarity computation.\n",
      "2024-11-26 13:18:19,052 [INFO] Computing tag similarity matrices using Jaccard similarity for Non-AI models...\n",
      "Computing Tag Similarity: 100%|██████████| 477753/477753 [00:00<00:00, 695668.57it/s]\n",
      "2024-11-26 13:18:19,745 [INFO] Completed tag similarity computation.\n",
      "2024-11-26 13:18:19,746 [INFO] Computing tag similarity matrices using Jaccard similarity for AI models...\n",
      "Computing Tag Similarity: 100%|██████████| 144991/144991 [00:00<00:00, 784010.20it/s]\n",
      "2024-11-26 13:18:19,937 [INFO] Completed tag similarity computation.\n",
      "2024-11-26 13:18:19,937 [INFO] Computing name similarity for all models...\n",
      "2024-11-26 13:18:19,940 [INFO] Computing name similarity using TF-IDF and cosine similarity...\n",
      "2024-11-26 13:18:19,961 [INFO] Completed name similarity computation.\n",
      "2024-11-26 13:18:19,963 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:19,974 [INFO] Threshold set at mean (0.0245) + 2*std (0.0437) = 0.1118\n",
      "2024-11-26 13:18:19,974 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:19,982 [INFO] Threshold set at mean (0.0064) + 2*std (0.0287) = 0.0639\n",
      "2024-11-26 13:18:19,983 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:19,990 [INFO] Threshold set at mean (0.3394) + 2*std (0.1204) = 0.5803\n",
      "2024-11-26 13:18:19,991 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:20,011 [INFO] Threshold set at mean (0.0171) + 2*std (0.0490) = 0.1150\n",
      "2024-11-26 13:18:20,013 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:20,017 [INFO] Threshold set at mean (0.0380) + 2*std (0.0515) = 0.1409\n",
      "2024-11-26 13:18:20,018 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:20,021 [INFO] Threshold set at mean (0.0023) + 2*std (0.0168) = 0.0360\n",
      "2024-11-26 13:18:20,022 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:20,025 [INFO] Threshold set at mean (0.4552) + 2*std (0.1165) = 0.6881\n",
      "2024-11-26 13:18:20,026 [INFO] Determining threshold based on mean + 2*std of similarity scores...\n",
      "2024-11-26 13:18:20,046 [INFO] Threshold set at mean (0.0171) + 2*std (0.0490) = 0.1150\n",
      "2024-11-26 13:18:20,048 [INFO] Constructing knowledge graph with embeddings...\n",
      "2024-11-26 13:18:20,049 [INFO] Creating Non-AI nodes in Neo4j with embeddings...\n",
      "2024-11-26 13:18:20,053 [INFO] Use pytorch device_name: cpu\n",
      "2024-11-26 13:18:20,054 [INFO] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "C:\\Users\\noski\\PycharmProjects\\Semantic_Search_Thingiverse\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7343b2ea6cb46418eeb2ff1653fb0e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Nodes: 100%|██████████| 978/978 [00:00<00:00, 31027.56it/s]\n",
      "Merging Nodes in Batches: 100%|██████████| 1/1 [00:04<00:00,  4.43s/it]\n",
      "2024-11-26 13:18:43,448 [INFO] Completed creating Non-AI nodes with embeddings in Neo4j.\n",
      "2024-11-26 13:18:43,452 [INFO] Creating AI nodes in Neo4j with embeddings...\n",
      "2024-11-26 13:18:43,455 [INFO] Use pytorch device_name: cpu\n",
      "2024-11-26 13:18:43,457 [INFO] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f12674ecae14244a5a6d920a696ef76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Nodes: 100%|██████████| 539/539 [00:00<00:00, 31507.99it/s]\n",
      "Merging Nodes in Batches: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "2024-11-26 13:18:55,672 [INFO] Completed creating AI nodes with embeddings in Neo4j.\n",
      "2024-11-26 13:18:55,676 [INFO] Creating Non-AI similarity edges in Neo4j in batches...\n",
      "2024-11-26 13:18:55,678 [INFO] Processing Non-AI relationship type: SIMILAR_SUMMARY_NONAI\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:   0%|          | 0/477753 [00:00<?, ?it/s]2024-11-26 13:19:46,109 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  10%|▉         | 46658/477753 [01:06<07:45, 925.23it/s]2024-11-26 13:20:36,650 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  17%|█▋        | 83495/477753 [01:56<08:06, 809.62it/s]2024-11-26 13:21:27,508 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  26%|██▌       | 123479/477753 [02:46<07:23, 798.87it/s]2024-11-26 13:22:16,384 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  37%|███▋      | 175510/477753 [03:36<05:35, 901.49it/s]2024-11-26 13:23:05,777 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  44%|████▍     | 210213/477753 [04:26<05:22, 830.29it/s]2024-11-26 13:23:53,580 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  52%|█████▏    | 247900/477753 [05:16<04:41, 816.42it/s]2024-11-26 13:24:41,303 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  58%|█████▊    | 277922/477753 [05:56<04:24, 756.33it/s]2024-11-26 13:25:27,347 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  66%|██████▋   | 316690/477753 [06:46<03:25, 782.59it/s]2024-11-26 13:26:15,012 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  70%|███████   | 335454/477753 [07:36<03:35, 661.26it/s]2024-11-26 13:27:01,679 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  75%|███████▍  | 357467/477753 [08:16<03:19, 603.69it/s]2024-11-26 13:27:49,921 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  82%|████████▏ | 391399/477753 [09:06<02:16, 634.56it/s]2024-11-26 13:28:35,323 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  88%|████████▊ | 421741/477753 [09:56<01:26, 644.47it/s]2024-11-26 13:29:20,512 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  91%|█████████▏| 436257/477753 [10:36<01:15, 549.19it/s]2024-11-26 13:30:05,817 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  93%|█████████▎| 442282/477753 [11:26<01:23, 425.64it/s]2024-11-26 13:30:51,393 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI:  94%|█████████▍| 451248/477753 [12:06<01:14, 357.19it/s]2024-11-26 13:31:36,664 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_NONAI: 100%|██████████| 477753/477753 [12:40<00:00, 627.81it/s]\n",
      "2024-11-26 13:32:01,577 [INFO] Inserted final batch of 548 relationships for SIMILAR_SUMMARY_NONAI.\n",
      "2024-11-26 13:32:01,578 [INFO] Total edges created for SIMILAR_SUMMARY_NONAI: 16548\n",
      "2024-11-26 13:32:01,579 [INFO] Processing Non-AI relationship type: SIMILAR_TAGS_NONAI\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:   0%|          | 0/477753 [00:00<?, ?it/s]2024-11-26 13:32:49,008 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:   9%|▉         | 42480/477753 [01:01<08:05, 895.66it/s]2024-11-26 13:33:34,137 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  17%|█▋        | 80127/477753 [01:51<07:42, 860.26it/s]2024-11-26 13:34:20,787 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  23%|██▎       | 110076/477753 [02:31<08:03, 759.92it/s]2024-11-26 13:35:07,409 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  34%|███▍      | 161253/477753 [03:21<05:54, 893.77it/s]2024-11-26 13:35:53,243 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  42%|████▏     | 199862/477753 [04:11<05:17, 875.39it/s]2024-11-26 13:36:40,843 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  50%|████▉     | 238766/477753 [04:51<04:39, 855.25it/s]2024-11-26 13:37:28,422 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  56%|█████▋    | 269608/477753 [05:41<04:24, 786.72it/s]2024-11-26 13:38:19,002 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  62%|██████▏   | 297137/477753 [06:31<04:15, 705.61it/s]2024-11-26 13:39:06,180 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  69%|██████▉   | 328983/477753 [07:21<03:33, 696.17it/s]2024-11-26 13:39:51,685 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  75%|███████▍  | 357467/477753 [08:01<02:58, 675.23it/s]2024-11-26 13:40:39,728 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  81%|████████  | 387657/477753 [08:51<02:16, 660.70it/s]2024-11-26 13:41:26,156 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  87%|████████▋ | 414210/477753 [09:41<01:40, 634.08it/s]2024-11-26 13:42:14,715 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  90%|█████████ | 431539/477753 [10:31<01:24, 548.31it/s]2024-11-26 13:43:04,935 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  91%|█████████▏| 436465/477753 [11:21<01:41, 407.07it/s]2024-11-26 13:43:55,640 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  92%|█████████▏| 441402/477753 [12:11<01:57, 310.59it/s]2024-11-26 13:44:43,181 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  94%|█████████▍| 449551/477753 [12:51<01:44, 269.61it/s]2024-11-26 13:45:31,218 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI:  96%|█████████▌| 459618/477753 [13:41<01:12, 251.71it/s]2024-11-26 13:46:19,219 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_NONAI.\n",
      "Processing Pairs for SIMILAR_TAGS_NONAI: 100%|██████████| 477753/477753 [14:17<00:00, 557.06it/s]\n",
      "2024-11-26 13:46:23,446 [INFO] Inserted final batch of 88 relationships for SIMILAR_TAGS_NONAI.\n",
      "2024-11-26 13:46:23,448 [INFO] Total edges created for SIMILAR_TAGS_NONAI: 18088\n",
      "2024-11-26 13:46:23,449 [INFO] Processing Non-AI relationship type: SIMILAR_EMBEDDING_NONAI\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:   0%|          | 0/477753 [00:00<?, ?it/s]2024-11-26 13:47:13,589 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:  18%|█▊        | 87772/477753 [01:09<03:42, 1750.60it/s]2024-11-26 13:48:01,726 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:  34%|███▍      | 164656/477753 [01:50<03:08, 1661.90it/s]2024-11-26 13:48:51,176 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:  46%|████▌     | 219688/477753 [02:40<03:03, 1409.84it/s]2024-11-26 13:49:38,293 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:  54%|█████▍    | 257280/477753 [03:30<03:07, 1174.49it/s]2024-11-26 13:50:26,799 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:  62%|██████▏   | 297460/477753 [04:20<02:51, 1049.44it/s]2024-11-26 13:51:17,779 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:  73%|███████▎  | 347964/477753 [05:10<02:06, 1028.75it/s]2024-11-26 13:52:06,541 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI:  86%|████████▋ | 412787/477753 [06:00<00:57, 1126.34it/s]2024-11-26 13:52:55,207 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_NONAI: 100%|██████████| 477753/477753 [06:31<00:00, 1219.48it/s]\n",
      "2024-11-26 13:53:33,012 [INFO] Inserted final batch of 761 relationships for SIMILAR_EMBEDDING_NONAI.\n",
      "2024-11-26 13:53:33,013 [INFO] Total edges created for SIMILAR_EMBEDDING_NONAI: 8761\n",
      "2024-11-26 13:53:33,013 [INFO] Processing Non-AI relationship type: SIMILAR_NAMES_ALL\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:   0%|          | 0/477753 [00:00<?, ?it/s]2024-11-26 13:54:25,005 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  16%|█▌        | 74567/477753 [01:10<04:41, 1434.22it/s]2024-11-26 13:55:13,226 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  25%|██▌       | 119791/477753 [01:50<05:10, 1151.34it/s]2024-11-26 13:56:00,540 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  39%|███▉      | 187881/477753 [02:40<03:46, 1279.14it/s]2024-11-26 13:56:48,458 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  51%|█████     | 242802/477753 [03:30<03:11, 1227.12it/s]2024-11-26 13:57:36,465 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  60%|██████    | 288950/477753 [04:20<02:46, 1131.70it/s]2024-11-26 13:58:24,250 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  67%|██████▋   | 321808/477753 [05:10<02:38, 981.64it/s] 2024-11-26 13:59:11,237 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  74%|███████▎  | 351504/477753 [05:50<02:25, 869.10it/s]2024-11-26 13:59:57,946 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  81%|████████▏ | 388395/477753 [06:41<01:45, 844.22it/s]2024-11-26 14:00:45,145 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  89%|████████▉ | 426435/477753 [07:31<01:01, 832.30it/s]2024-11-26 14:01:33,758 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  90%|█████████ | 431859/477753 [08:11<01:15, 605.69it/s]2024-11-26 14:02:22,217 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  92%|█████████▏| 437412/477753 [09:01<01:28, 453.80it/s]2024-11-26 14:03:09,836 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL:  93%|█████████▎| 445937/477753 [09:51<01:25, 370.62it/s]2024-11-26 14:04:00,550 [INFO] Inserted batch of 1000 relationships for SIMILAR_NAMES_ALL.\n",
      "Processing Pairs for SIMILAR_NAMES_ALL: 100%|██████████| 477753/477753 [10:27<00:00, 761.30it/s]\n",
      "2024-11-26 14:04:24,445 [INFO] Inserted final batch of 516 relationships for SIMILAR_NAMES_ALL.\n",
      "2024-11-26 14:04:24,447 [INFO] Total edges created for SIMILAR_NAMES_ALL: 13516\n",
      "2024-11-26 14:04:24,449 [INFO] Completed Non-AI edge creation in Neo4j.\n",
      "2024-11-26 14:04:24,451 [INFO] Creating AI similarity edges in Neo4j in batches...\n",
      "2024-11-26 14:04:24,454 [INFO] Processing AI relationship type: SIMILAR_SUMMARY_AI\n",
      "Processing Pairs for SIMILAR_SUMMARY_AI:   0%|          | 0/144991 [00:00<?, ?it/s]2024-11-26 14:05:12,517 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_AI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_AI:  19%|█▉        | 28138/144991 [00:59<03:19, 585.45it/s]2024-11-26 14:06:01,558 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_AI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_AI:  37%|███▋      | 53669/144991 [01:49<02:46, 546.99it/s]2024-11-26 14:06:50,172 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_AI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_AI:  52%|█████▏    | 74795/144991 [02:39<02:21, 495.67it/s]2024-11-26 14:07:38,033 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_AI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_AI:  65%|██████▍   | 93685/144991 [03:29<01:52, 456.18it/s]2024-11-26 14:08:26,484 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_AI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_AI:  78%|███████▊  | 113775/144991 [04:19<01:10, 441.18it/s]2024-11-26 14:09:13,865 [INFO] Inserted batch of 1000 relationships for SIMILAR_SUMMARY_AI.\n",
      "Processing Pairs for SIMILAR_SUMMARY_AI: 100%|██████████| 144991/144991 [04:49<00:00, 500.98it/s]\n",
      "2024-11-26 14:09:18,801 [INFO] Inserted final batch of 100 relationships for SIMILAR_SUMMARY_AI.\n",
      "2024-11-26 14:09:18,803 [INFO] Total edges created for SIMILAR_SUMMARY_AI: 6100\n",
      "2024-11-26 14:09:18,803 [INFO] Processing AI relationship type: SIMILAR_TAGS_AI\n",
      "Processing Pairs for SIMILAR_TAGS_AI:   0%|          | 0/144991 [00:00<?, ?it/s]2024-11-26 14:10:06,614 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_AI.\n",
      "Processing Pairs for SIMILAR_TAGS_AI:  35%|███▍      | 50425/144991 [01:05<01:29, 1054.76it/s]2024-11-26 14:10:56,086 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_AI.\n",
      "Processing Pairs for SIMILAR_TAGS_AI:  63%|██████▎   | 91196/144991 [01:55<00:58, 917.18it/s] 2024-11-26 14:11:44,613 [INFO] Inserted batch of 1000 relationships for SIMILAR_TAGS_AI.\n",
      "Processing Pairs for SIMILAR_TAGS_AI: 100%|██████████| 144991/144991 [02:25<00:00, 994.40it/s]\n",
      "2024-11-26 14:11:46,176 [INFO] Inserted final batch of 32 relationships for SIMILAR_TAGS_AI.\n",
      "2024-11-26 14:11:46,178 [INFO] Total edges created for SIMILAR_TAGS_AI: 3032\n",
      "2024-11-26 14:11:46,179 [INFO] Processing AI relationship type: SIMILAR_EMBEDDING_AI\n",
      "Processing Pairs for SIMILAR_EMBEDDING_AI:   0%|          | 0/144991 [00:00<?, ?it/s]2024-11-26 14:12:34,489 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_AI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_AI:  44%|████▍     | 63801/144991 [00:58<01:01, 1320.74it/s]2024-11-26 14:13:19,764 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_AI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_AI:  62%|██████▏   | 90006/144991 [01:48<01:01, 895.98it/s] 2024-11-26 14:14:05,097 [INFO] Inserted batch of 1000 relationships for SIMILAR_EMBEDDING_AI.\n",
      "Processing Pairs for SIMILAR_EMBEDDING_AI: 100%|██████████| 144991/144991 [02:18<00:00, 1043.65it/s]\n",
      "2024-11-26 14:14:16,883 [INFO] Inserted final batch of 257 relationships for SIMILAR_EMBEDDING_AI.\n",
      "2024-11-26 14:14:16,884 [INFO] Total edges created for SIMILAR_EMBEDDING_AI: 3257\n",
      "2024-11-26 14:14:16,885 [INFO] Completed AI edge creation in Neo4j.\n",
      "2024-11-26 14:14:16,886 [INFO] Pruning graph edges based on similarity thresholds...\n",
      "2024-11-26 14:14:17,386 [INFO] Pruned edges of type SIMILAR_SUMMARY_NONAI below threshold 0.11181324175307887.\n",
      "2024-11-26 14:14:17,710 [INFO] Pruned edges of type SIMILAR_TAGS_NONAI below threshold 0.06386230841430725.\n",
      "2024-11-26 14:14:17,837 [INFO] Pruned edges of type SIMILAR_EMBEDDING_NONAI below threshold 0.5802618116140366.\n",
      "2024-11-26 14:14:18,054 [INFO] Pruned edges of type SIMILAR_NAMES_ALL below threshold 0.11499804157089621.\n",
      "2024-11-26 14:14:18,274 [INFO] Pruned edges of type SIMILAR_SUMMARY_AI below threshold 0.1408808680914183.\n",
      "2024-11-26 14:14:18,369 [INFO] Pruned edges of type SIMILAR_TAGS_AI below threshold 0.0359668147317035.\n",
      "2024-11-26 14:14:18,464 [INFO] Pruned edges of type SIMILAR_EMBEDDING_AI below threshold 0.6880883276462555.\n",
      "2024-11-26 14:14:18,465 [INFO] Completed pruning of graph edges.\n",
      "2024-11-26 14:14:18,466 [INFO] Performing node clustering using NetworkX and Louvain method for summary_nonai...\n",
      "Adding Edges to Graph: 100%|██████████| 477753/477753 [00:00<00:00, 2122364.78it/s]\n",
      "2024-11-26 14:14:18,904 [INFO] Detected 17 communities for summary_nonai.\n",
      "2024-11-26 14:14:18,905 [INFO] Updating Neo4j nodes with community labels...\n",
      "Updating Communities: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "2024-11-26 14:14:19,602 [INFO] Completed updating community labels in Neo4j for summary_nonai.\n",
      "2024-11-26 14:14:19,603 [INFO] Performing node clustering using NetworkX and Louvain method for tags_nonai...\n",
      "Adding Edges to Graph: 100%|██████████| 477753/477753 [00:00<00:00, 2108325.73it/s]\n",
      "2024-11-26 14:14:20,120 [INFO] Detected 109 communities for tags_nonai.\n",
      "2024-11-26 14:14:20,122 [INFO] Updating Neo4j nodes with community labels...\n",
      "Updating Communities: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "2024-11-26 14:14:20,820 [INFO] Completed updating community labels in Neo4j for tags_nonai.\n",
      "2024-11-26 14:14:20,820 [INFO] Performing node clustering using NetworkX and Louvain method for embedding_nonai...\n",
      "Adding Edges to Graph: 100%|██████████| 477753/477753 [00:00<00:00, 1778058.16it/s]\n",
      "2024-11-26 14:14:21,272 [INFO] Detected 141 communities for embedding_nonai.\n",
      "2024-11-26 14:14:21,273 [INFO] Updating Neo4j nodes with community labels...\n",
      "Updating Communities: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "2024-11-26 14:14:21,911 [INFO] Completed updating community labels in Neo4j for embedding_nonai.\n",
      "2024-11-26 14:14:21,911 [INFO] Performing node clustering using NetworkX and Louvain method for names_all...\n",
      "Adding Edges to Graph:   0%|          | 981/1122751 [00:00<00:01, 1034081.99it/s]\n",
      "2024-11-26 14:14:21,924 [ERROR] Error during node clustering for names_all: list index out of range\n",
      "2024-11-26 14:14:21,925 [INFO] Performing node clustering using NetworkX and Louvain method for summary_ai...\n",
      "Adding Edges to Graph: 100%|██████████| 144991/144991 [00:00<00:00, 1988146.80it/s]\n",
      "2024-11-26 14:14:22,100 [INFO] Detected 11 communities for summary_ai.\n",
      "2024-11-26 14:14:22,101 [INFO] Updating Neo4j nodes with community labels...\n",
      "Updating Communities: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "2024-11-26 14:14:22,507 [INFO] Completed updating community labels in Neo4j for summary_ai.\n",
      "2024-11-26 14:14:22,507 [INFO] Performing node clustering using NetworkX and Louvain method for tags_ai...\n",
      "Adding Edges to Graph: 100%|██████████| 144991/144991 [00:00<00:00, 2057127.74it/s]\n",
      "2024-11-26 14:14:22,635 [INFO] Detected 59 communities for tags_ai.\n",
      "2024-11-26 14:14:22,636 [INFO] Updating Neo4j nodes with community labels...\n",
      "Updating Communities: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "2024-11-26 14:14:23,073 [INFO] Completed updating community labels in Neo4j for tags_ai.\n",
      "2024-11-26 14:14:23,073 [INFO] Performing node clustering using NetworkX and Louvain method for embedding_ai...\n",
      "Adding Edges to Graph: 100%|██████████| 144991/144991 [00:00<00:00, 1738998.10it/s]\n",
      "2024-11-26 14:14:23,254 [INFO] Detected 117 communities for embedding_ai.\n",
      "2024-11-26 14:14:23,255 [INFO] Updating Neo4j nodes with community labels...\n",
      "Updating Communities: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "2024-11-26 14:14:23,700 [INFO] Completed updating community labels in Neo4j for embedding_ai.\n",
      "2024-11-26 14:14:23,701 [INFO] Creating 'SIMILAR_NAMES' relationships based on extracted names...\n",
      "Processing Similar Names: 100%|██████████| 1469/1469 [00:00<00:00, 53379.94it/s]\n",
      "Inserting 'SIMILAR_NAMES' Relationships: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n",
      "2024-11-26 14:14:26,321 [INFO] Inserted total 'SIMILAR_NAMES' relationships: 55\n",
      "2024-11-26 14:14:26,321 [INFO] Knowledge graph construction complete.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lastly, this is the user search interface code for this combined system (model_search_by_sim_type.py). This allows the user to pick which edges they want to traverse:\n",
    "'1': 'SIMILAR_TAGS_NONAI',\n",
    "'2': 'SIMILAR_TAGS_AI',\n",
    "'3': 'SIMILAR_SUMMARY_NONAI',\n",
    "'4': 'SIMILAR_SUMMARY_AI',\n",
    "'5': 'SIMILAR_EMBEDDING_NONAI',\n",
    "'6': 'SIMILAR_EMBEDDING_AI',\n",
    "'7': 'SIMILAR_NAMES'\n",
    "\n",
    "### Only the main function and output are shown.\n",
    "\n",
    "The user can pick one of these or multiple and weigh them by importance. Then, advanced functions like filtering, picking, and doing further searches are included, just like in the model_search.py code.\n"
   ],
   "id": "2021130bd797a493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main function to handle user queries and perform search on the knowledge graph using multiple similarity edges.\n",
    "#     \"\"\"\n",
    "#     print(\"Initializing Model Search...\")\n",
    "#\n",
    "#     # Connect to Neo4j\n",
    "#     graph_db = connect_to_neo4j(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "#\n",
    "#     # Load Sentence Transformer model once\n",
    "#     logging.info(\"Loading SentenceTransformer model...\")\n",
    "#     model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#     logging.info(\"SentenceTransformer model loaded successfully.\")\n",
    "#\n",
    "#     # Fetch all models with embeddings and precompute summary embeddings\n",
    "#     df_models = fetch_all_models(graph_db, model)\n",
    "#\n",
    "#     while True:\n",
    "#         # Prompt user for input\n",
    "#         user_query = input(\"\\nEnter your search query (or type 'exit' to quit): \").strip()\n",
    "#         if user_query.lower() == 'exit':\n",
    "#             print(\"Exiting Model Search. Goodbye!\")\n",
    "#             break\n",
    "#         if not user_query:\n",
    "#             print(\"Empty query. Please enter a valid search term.\")\n",
    "#             continue\n",
    "#\n",
    "#         # Prompt user to select similarity types\n",
    "#         selected_similarity_types = prompt_similarity_types()\n",
    "#\n",
    "#         # Prompt user to assign weights to the selected similarity types\n",
    "#         similarity_weights = prompt_similarity_weights(selected_similarity_types)\n",
    "#\n",
    "#         preprocessed_query = user_query  # No preprocessing needed\n",
    "#         if not preprocessed_query.strip():\n",
    "#             print(\"Invalid query. Please enter a non-empty search term.\")\n",
    "#             continue\n",
    "#\n",
    "#         # Extract tags from the query (assuming tags are the main nouns)\n",
    "#         doc = nlp(user_query)\n",
    "#         query_tags = set([token.text.lower() for token in doc if token.pos_ == 'NOUN'])\n",
    "#\n",
    "#         # Aggregate similarity scores\n",
    "#         logging.info(\"Aggregating similarity scores...\")\n",
    "#         df_aggregated = aggregate_similarities(\n",
    "#             df_models,\n",
    "#             similarity_weights,\n",
    "#             selected_similarity_types,\n",
    "#             query_tags,\n",
    "#             preprocessed_query,\n",
    "#             model\n",
    "#         )\n",
    "#\n",
    "#         if df_aggregated.empty:\n",
    "#             print(\"No similar models found based on the selected similarity types.\")\n",
    "#             continue\n",
    "#\n",
    "#         # Display the top-k results\n",
    "#         top_k = 10  # Number of top results to retrieve\n",
    "#         top_results = display_results(df_aggregated, top_k=top_k)\n",
    "#\n",
    "#         if top_results is None or top_results.empty:\n",
    "#             print(\"No similar models to display.\")\n",
    "#             continue\n",
    "#\n",
    "#         # Initialize the current search scope with the initial top results\n",
    "#         current_scope = top_results\n",
    "#\n",
    "#         while True:\n",
    "#             # Prompt user to apply further filters or select a model\n",
    "#             selected = prompt_filter(current_scope)\n",
    "#             if selected == 'yes':\n",
    "#                 # User wants to filter further\n",
    "#                 # Prompt to select a model from current_scope\n",
    "#                 try:\n",
    "#                     selection = int(input(\n",
    "#                         f\"Select a model to view similar models by entering its rank (1-{len(current_scope)}): \").strip())\n",
    "#                     if 1 <= selection <= len(current_scope):\n",
    "#                         selected_model_id = current_scope.iloc[selection - 1]['id']\n",
    "#                         logging.info(f\"Selected Model ID for further filtering: {selected_model_id}\")\n",
    "#                     else:\n",
    "#                         print(f\"Please enter a number between 1 and {len(current_scope)}.\")\n",
    "#                         continue\n",
    "#                 except ValueError:\n",
    "#                     print(\"Invalid input. Please enter a valid number.\")\n",
    "#                     continue\n",
    "#\n",
    "#                 # Fetch model details\n",
    "#                 selected_details = fetch_model_details(graph_db, selected_model_id)\n",
    "#                 if not selected_details:\n",
    "#                     print(f\"No model found with ID: {selected_model_id}.\")\n",
    "#                     continue\n",
    "#\n",
    "#                 # Prompt user to select similarity types for further filtering\n",
    "#                 print(\"\\nFor further filtering, you can select similarity types again.\")\n",
    "#                 further_similarity_types = prompt_similarity_types()\n",
    "#\n",
    "#                 # Prompt user to assign weights to the selected similarity types\n",
    "#                 further_similarity_weights = prompt_similarity_weights(further_similarity_types)\n",
    "#\n",
    "#                 # Extract tags from the selected model's name and descriptions\n",
    "#                 selected_model_tags = set()\n",
    "#                 if pd.notnull(selected_details['tags_nonai']):\n",
    "#                     selected_model_tags.update(\n",
    "#                         [tag.strip().lower() for tag in selected_details['tags_nonai'].split(',') if tag.strip()])\n",
    "#                 if pd.notnull(selected_details['tags_ai']):\n",
    "#                     selected_model_tags.update([tag.strip().lower() for tag in selected_details['tags_ai'].split(',') if tag.strip()])\n",
    "#\n",
    "#                 # Since descriptions are preprocessed, use them directly\n",
    "#                 selected_model_summary = selected_details['description_nonai'] if pd.notnull(\n",
    "#                     selected_details['description_nonai']) else selected_details['description_ai']\n",
    "#\n",
    "#                 # Aggregate similarity scores based on the selected model's attributes\n",
    "#                 logging.info(\"Aggregating similarity scores for further filtering...\")\n",
    "#                 df_new_similar = aggregate_similarities(\n",
    "#                     df_models,\n",
    "#                     further_similarity_weights,\n",
    "#                     further_similarity_types,\n",
    "#                     selected_model_tags,\n",
    "#                     selected_model_summary,\n",
    "#                     model\n",
    "#                 )\n",
    "#\n",
    "#                 if df_new_similar.empty:\n",
    "#                     print(\"No further models found based on your selection.\")\n",
    "#                     break  # Exit filtering loop\n",
    "#\n",
    "#                 # Display the new set of similar models\n",
    "#                 print(f\"\\nModels similar to Model ID: {selected_model_id}:\")\n",
    "#                 similar_top_k = 10  # Number of top similar models to display\n",
    "#                 similar_top_results = display_results(df_new_similar, top_k=similar_top_k)\n",
    "#\n",
    "#                 if similar_top_results is None or similar_top_results.empty:\n",
    "#                     print(\"No similar models found in this step.\")\n",
    "#                     break  # Exit filtering loop\n",
    "#\n",
    "#                 # Update the current scope to the new similar models\n",
    "#                 current_scope = similar_top_results\n",
    "#\n",
    "#             elif selected == 'no':\n",
    "#                 # User does not want to filter further\n",
    "#                 break  # Exit filtering loop\n",
    "#             else:\n",
    "#                 # User selected a model by rank to view its full summary\n",
    "#                 selected_model_id = selected\n",
    "#                 # Fetch the full details of the selected model\n",
    "#                 selected_details = fetch_model_details(graph_db, selected_model_id)\n",
    "#                 if selected_details:\n",
    "#                     print(f\"\\n--- Full Summary of Selected Model ---\")\n",
    "#                     print(f\"ID: {selected_details['id']}\")\n",
    "#                     print(f\"Name: {selected_details['name'] if pd.notnull(selected_details['name']) else 'N/A'}\")\n",
    "#                     print(\n",
    "#                         f\"Tags Non-AI: {selected_details['tags_nonai'] if pd.notnull(selected_details['tags_nonai']) else 'N/A'}\")\n",
    "#                     print(\n",
    "#                         f\"Tags AI: {selected_details['tags_ai'] if pd.notnull(selected_details['tags_ai']) else 'N/A'}\")\n",
    "#                     print(\n",
    "#                         f\"Description Non-AI: {selected_details['description_nonai'] if pd.notnull(selected_details['description_nonai']) else 'N/A'}\")\n",
    "#                     print(\n",
    "#                         f\"Description AI: {selected_details['description_ai'] if pd.notnull(selected_details['description_ai']) else 'N/A'}\")\n",
    "#                     print(f\"---------------------------------------\\n\")\n",
    "#                 else:\n",
    "#                     print(f\"No details found for Model ID: {selected_model_id}.\")\n",
    "#\n",
    "#                 # After displaying the summary, ask if the user wants another search\n",
    "#                 while True:\n",
    "#                     another_search = input(\"Do you want to perform another search? (yes/no): \").strip().lower()\n",
    "#                     if another_search in ['yes', 'y']:\n",
    "#                         break  # Break to the main search loop\n",
    "#                     elif another_search in ['no', 'n']:\n",
    "#                         print(\"Exiting Model Search. Goodbye!\")\n",
    "#                         sys.exit(0)\n",
    "#                     else:\n",
    "#                         print(\"Please answer with 'yes' or 'no'.\")\n",
    "#                 break  # Exit filtering loop to perform another search\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ],
   "id": "7294b5a4fcd34a85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
